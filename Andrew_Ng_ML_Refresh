Andrew Ng, Founder of Coursera conducted a Machine Learning course. Below are the class notes taken after watching videos.

### Machine Learning Definition:
  Let computer learn perform a Task T for a peformance metric P and obtain an experience E, based on the available data.
  When a new data point arises, it performances the Task T, based on its historical Experience
  
  
### Types of Machine Learning
# Supervised Learning
  Ex: Classifying email as spam or not.
      Classifying handwritten digits
      Classifying Fraud/Not Fraud Txns and so on
      
      
# Unsupervised Learning
  Ex: We let computer cluster the data points on some similarity. Market Segmentation, Nearest Neighbourhood house prices
  
# Other types of Machine learning: Reinforcement learning (chess/checker), recommendation learning(Netflix video recommendation, 
Amazon product recommendation, Google ads)
 
# Class 3
## Supervised Learning: Examples are provided with question and answer and we let the computer learn.
### Regression Model: Goal is to predict the continous Dependent Variable
  Ex: Predict housing prices based on location, house features etc
### Classification Model: Goal is to predict the Discrete values
  Ex: Predict the lump as benign or malignant

### SVM (Support Vector Machine) is good at prediction, when it comes to dealing with lots and lots of features.

# Class 4
## Unsupervised Learning: We let the computer let group based on data
Examples: Market Segmentation, News grouping on Google, Genetics of similar people, Astronomical Data analysis

### Octave is good for rapid prototyping and then the models can be created in C++/Java environment.

# Class 5
## Linear Regression.
  Regression is predicting the continous variable. Hypothesis is a function that modifies independent variables and 
  produces dependent variables.
  
  y = h(x) = q0 + q1*x
  
  Linear regression with a single variable is also called univariate linear regression.
  It is linear in terms of Coefficients not in terms of independent variables.
  
  
 # Class 6
 Cost function: is the function that we are trying to minimize, by choosing the right values of parameters(q0, q1).
 J(q0, q1) = 1/2m * Sum ( h(x) - y)^2
 i.e RMSE is the cost function.
 
 
# Class 7 
Gradient Descent

q0, q1 = Parameters
Alpha = Learning rate
J(q0, q1) = Cost Function
Goal is to minimize Cost Function, by choosing the right parameters.

==
1. Based on the starting point (initialized values of parameters), we reach different local minima
2. Alpha = Learning Rate
3. Update all parameters at once
qi := qi - alpha * partial derivative qi ( J(q0,q1))
4. repeat this until, there is no improvement in Cost function for certain iterations

# Class 8
There is a alternate way to find out the parameters without recursively performing Gradient Descent.
It is called Natural ?? using Matrices and vectors

Matrices are represented with capital letters.
Vectors are represented with small letters.


